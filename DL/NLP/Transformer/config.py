N_LAYERS = 4
D_MODEL = 128
N_HEADS = 8
UNITS = 128
DROP = 0.1

EPOCHS = 100000
MAX_SENTENCE_LENGTH = 20
MIN_SENTENCE_LENGTH = 5
SAV_P = 1

# SAVE_PERIOD
TGT_VOC_SIZE = 1024 * 4
DATA_BUFFER_SIZE = 10240

# TOK_PATH = 'Save/Chat/tokenizer'
# TOK_PATH = 'Save/Chat_CN/tokenizer'
TOK_PATH = 'Save/Translate/tokenizer'
# WGT_PATH = 'Save/Chat/bot_4'
# WGT_PATH = 'Save/Chat_CN/bot_4'
WGT_PATH = 'Save/Translate/bot_4'

STD_TCOUNT = 1000
SET_TCOUNT = 5000
# TCOUNT = TRAIN_COUNT
STD_BS = 128
SET_BS = 8
# BS = BATCH_SIZE
STD_TRAIN_STEP = 8

SET_TRAIN_STEP = int(STD_TRAIN_STEP * (SET_TCOUNT / STD_TCOUNT) / (SET_BS / STD_BS))
WARM_UP_EPOCH = max(1, int(SET_TRAIN_STEP * 0.25))
SAV_STP = SET_TRAIN_STEP * SAV_P
