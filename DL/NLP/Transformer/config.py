N_LAYERS = 4
D_MODEL = 256
N_HEADS = 8
UNITS = 256
DROP = 0.1
STD_BS = 16
SET_BS = 128
# BS = BATCH_SIZE
EPOCHS = 1000
MAX_SENTENCE_LENGTH = 20
MIN_SENTENCE_LENGTH = 5
SAV_P = 1

# SAVE_PERIOD
TGT_VOC_SIZE = 1024 * 4
DATA_BUFFER_SIZE = 10240
# TOK_PATH = 'Save/Chat/tokenizer'
# TOK_PATH = 'Save/Chat_CN/tokenizer'
TOK_PATH = 'Save/Translate/tokenizer'
# WGT_PATH = 'Save/Chat/bot_4'
# WGT_PATH = 'Save/Chat_CN/bot_4'
WGT_PATH = 'Save/Translate/bot_4'
STD_TCOUNT = 625
SET_TCOUNT = 5000
# TCOUNT = TRAIN_COUNT
STD_TRAIN_STEP = 6
SET_TRAIN_STEP = int(STD_TRAIN_STEP * (SET_TCOUNT / STD_TCOUNT) / (SET_BS / STD_BS))
WARM_UP_EPOCH = max(1, int(SET_TRAIN_STEP * 0.25))
SAV_STP= SET_TRAIN_STEP * SAV_P
