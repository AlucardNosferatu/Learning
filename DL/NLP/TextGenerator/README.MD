# 三个臭皮匠
使用一些小模型通过业务逻辑缝合成一个端到端模型的实验
# 目的
### 模型简化
使用更小的模型，将减轻对算力的依赖，也能加快迭代调优
### 模块分割
将模型分拆成几个部分，更有利于模型功能的扩展和替换升级
### 业务关联
通过将大模型的功能分拆为小模型的组合得到相对可解释的中间变量，使模型输出更容易被业务控制
# 基本思路
### 文本分类
对用户输入的文本进行识别得到分类标签
#### 训练数据
demo使用Kmeans对word2vec词向量序列进行聚类（目前是聚10类），得到初始数据中心点

后续新文本序列加入时使用保存的word2vec词向量矩阵转化为词向量序列，根据其与几个数据中心点的距离选择最近的一个进行标记

实际应用场景中应该根据业务需求人工打标
#### 分类模型
非常基本简单的CNN分类（要的就是简单）
### 业务逻辑
根据给定的输入分类标签人为选择输出分类标签（也可用深度学习模型自动选择）
### 文本生成
根据给定的输出分类标签生成
#### 训练数据
demo中暂时与**文本分类**采用相同的训练数据

Word2Vec词向量维数为32，与训练数据中的最大句长（pad后的句长）相同，这是为保证后续生成模型的生成网络能输出方阵
#### 生成模型
由Keras官方基于CGAN的MNIST手写数字生成改造而来

生成网络：

输入.shape=(噪声维数64+分类10) 输出.shape=(词向量维数32,带pad的句子长度32,通道1)

判别网络：

输入.shape=(词向量维数32,带pad的句子长度32,通道1+分类10) 输出.shape=(真文本的概率1)

注1：判别网络输入第三个维度index=0以后的张量由分类填充而来，除去分类index所在的slice是32×32的全1矩阵，其它都是32×32的全0矩阵

注2：原生成网络采用2D反卷积来生成图像矩阵，现已改为1D反卷积（除去最后一层外都是时序上卷积，只有最后一层对单个时刻的编码做词向量维度的2D反卷积）
# 2023.04.14
生成模型训练的效果非常差，再加上单位工作量增加导致无法分配更多精力至此项目，此项目已暂停。

目前排除最常见的原因，也就是OneHot编码导致的离散数据梯度截断，因为这里生成模型的生成网络产生的根本就不是OneHot编码，而是Word2Vec的词向量序列方阵

目前考虑以下改进方案：

*改变生成网络的超参数（估计没用）*

*将生成网络改为自回归模型，每次只生成一个词向量，每次把上一时刻的词向量和分类一起输入，判别网络依然是对整个序列构成的矩阵进行判别*

*将判别网络换成基于蒙特卡洛树搜索的策略梯度（SeqGAN）*

*试试VAE*
